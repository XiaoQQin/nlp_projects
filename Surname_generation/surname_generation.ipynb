{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from argparse import Namespace\n",
    "from collections import Counter\n",
    "import json\n",
    "import re\n",
    "import string\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary(object):\n",
    "    \n",
    "    def __init__(self,token_to_idx=None):\n",
    "        if token_to_idx is None:\n",
    "            token_to_idx={}\n",
    "        self.token_to_idx=token_to_idx\n",
    "        self.idx_to_token={v:k for k,v in self.token_to_idx.items()}\n",
    "    \n",
    "    def to_serializable(self):\n",
    "        \n",
    "        return {'token_to_idx':self.token_to_idx}\n",
    "    @classmethod\n",
    "    def from_serializable(cls,contents):\n",
    "        return cls(**contents)\n",
    "    \n",
    "    def add_token(self,token):\n",
    "        \n",
    "        if token in self.token_to_idx:\n",
    "            index=self.token_to_idx[token]\n",
    "        else:\n",
    "            index=len(self.token_to_idx)\n",
    "            self.token_to_idx[token]=index\n",
    "            self.idx_to_token[index]=token\n",
    "        \n",
    "        return index\n",
    "    \n",
    "    def add_many(self,tokens):\n",
    "        return [self.add_token(token) for token in tokens]\n",
    "    \n",
    "    def lookup_token(self,token):\n",
    "        return self.token_to_idx[token]\n",
    "    \n",
    "    def lookup_idx(self,idx):\n",
    "        if idx not in self.idx_to_token:\n",
    "            raise KeyError(\"the index (%d) is not in the Vocabulary\" % index)\n",
    "        return self.idx_to_token[idx]\n",
    "    def __str__(self):\n",
    "        return \"<Vocabulary(size=%d)>\" % len(self)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.token_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequenceVocabulary(Vocabulary):\n",
    "    \n",
    "    def __init__(self,token_to_idx=None, unk_token=\"<UNK>\",\n",
    "                 mask_token=\"<MASK>\", begin_seq_token=\"<BEGIN>\",\n",
    "                 end_seq_token=\"<END>\"):\n",
    "        \n",
    "        super(SequenceVocabulary,self).__init__(token_to_idx)\n",
    "        \n",
    "        self._mask_token = mask_token\n",
    "        self._unk_token = unk_token\n",
    "        self._begin_seq_token = begin_seq_token\n",
    "        self._end_seq_token = end_seq_token\n",
    "        \n",
    "        \n",
    "        self.mask_index = self.add_token(self._mask_token)\n",
    "        self.unk_index = self.add_token(self._unk_token)\n",
    "        self.begin_seq_index = self.add_token(self._begin_seq_token)\n",
    "        self.end_seq_index = self.add_token(self._end_seq_token)\n",
    "        \n",
    "    \n",
    "    def to_serializable(self):\n",
    "        contents = super(SequenceVocabulary, self).to_serializable()\n",
    "        contents.update({'unk_token': self._unk_token,\n",
    "                         'mask_token': self._mask_token,\n",
    "                         'begin_seq_token': self._begin_seq_token,\n",
    "                         'end_seq_token': self._end_seq_token})\n",
    "        return contents\n",
    "    \n",
    "    def lookup_token(self, token):\n",
    "        \n",
    "        if self.unk_index>=0:\n",
    "            return self.token_to_idx.get(token,self.unk_index)\n",
    "        else:\n",
    "            return self.token_to_idx[token]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SurnamesVectorizer(object):\n",
    "    \n",
    "    def __init__(self,char_vocab,nationality_vocab):\n",
    "        self.char_vocab=char_vocab\n",
    "        self.nationality_vocab=nationality_vocab\n",
    "    \n",
    "    \n",
    "    def vectorize(self,surname,vector_length=-1):\n",
    "        \"\"\"\n",
    "        from_vector  包括所有token 的idx 和 begin_seq_idx ，不包括end_seq_index（假设为1）\n",
    "        to_vector    包括所有token的idx 和end_seq_index 不包括 begin_seq_dix（假设为0）\n",
    "        那么对其之后下面的token即为上面要预测（生成）的token\n",
    "            0  2 3 4 5\n",
    "            2  3 4 5 1\n",
    "        \"\"\"\n",
    "        #获取该单词的每个字符对应的序号\n",
    "        indices = [self.char_vocab.begin_seq_index] \n",
    "        indices.extend(self.char_vocab.lookup_token(token) for token in surname)\n",
    "        indices.append(self.char_vocab.end_seq_index)\n",
    "        \n",
    "        #如果未初始化\n",
    "        if vector_length < 0:\n",
    "            vector_length = len(indices) - 1\n",
    "            \n",
    "        #from vector\n",
    "        from_vector = np.empty(vector_length, dtype=np.int64)\n",
    "        from_indices=indices[:-1]\n",
    "        from_vector[:len(from_indices)]=from_indices\n",
    "        from_vector[len(from_indices):]=self.char_vocab.mask_index\n",
    "        \n",
    "        #to_vector\n",
    "        to_vector=np.empty(vector_length,dtype=np.int64)\n",
    "        to_indices=indices[1:]\n",
    "        to_vector[:len(to_indices)]=to_indices\n",
    "        to_vector[len(to_indices):]=self.char_vocab.mask_index\n",
    "        \n",
    "        return from_vector,to_vector\n",
    "    \n",
    "    @classmethod\n",
    "    def from_dataframe(cls,surname_df):\n",
    "        \n",
    "        char_vocab=SequenceVocabulary()\n",
    "        nationality_vocab=Vocabulary()\n",
    "        \n",
    "        for index,row in surname_df.iterrows():\n",
    "            for char in row.surname:\n",
    "                char_vocab.add_token(char)\n",
    "            nationality_vocab.add_token(row.nationality)\n",
    "        \n",
    "        return cls(char_vocab,nationality_vocab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SurnameDataset(Dataset):\n",
    "    \n",
    "    \"\"\"\n",
    "    创建数据集格式\n",
    "    \"\"\"\n",
    "    def __init__(self,surname_df,vectorizer):\n",
    "        \n",
    "        super(SurnameDataset,self).__init__()\n",
    "        self.surname_df=surname_df\n",
    "        self.vectorizer=vectorizer\n",
    "        \n",
    "        self._max_seq_length=max(map(len,self.surname_df.surname))+2\n",
    "        \n",
    "        self.train_df = self.surname_df[self.surname_df.split=='train']\n",
    "        self.train_size = len(self.train_df)\n",
    "\n",
    "        self.val_df = self.surname_df[self.surname_df.split=='val']\n",
    "        self.validation_size = len(self.val_df)\n",
    "\n",
    "        self.test_df = self.surname_df[self.surname_df.split=='test']\n",
    "        self.test_size = len(self.test_df)\n",
    "\n",
    "        self._lookup_dict = {'train': (self.train_df, self.train_size), \n",
    "                             'val': (self.val_df, self.validation_size), \n",
    "                             'test': (self.test_df, self.test_size)}\n",
    "        \n",
    "        self.set_split('train')\n",
    "    \n",
    "    @classmethod\n",
    "    def load_dataset_and_make_vectorizer(cls, surname_csv):\n",
    "        \"\"\"\n",
    "        从 csv文件中加\n",
    "        \"\"\"\n",
    "        surname_df = pd.read_csv(surname_csv)\n",
    "        return cls(surname_df, SurnamesVectorizer.from_dataframe(surname_df))\n",
    "    \n",
    "    def get_vectorizer(self):\n",
    "        return self.vectorizer\n",
    "    \n",
    "    def set_split(self,split='train'):\n",
    "        self._target_split=split\n",
    "        self._target_df,self._target_size=self._lookup_dict[split]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self._target_size\n",
    "    \n",
    "    def __getitem__(self,index):\n",
    "        \n",
    "        row=self._target_df.iloc[index]\n",
    "        from_vector,to_vector=self.vectorizer.vectorize(row.surname,self._max_seq_length)\n",
    "        nationality_index=self.vectorizer.nationality_vocab.lookup_token(row.nationality)\n",
    "        return {\n",
    "            'x_data':from_vector,\n",
    "            'y_target':to_vector,\n",
    "            'class_index':nationality_index\n",
    "        }\n",
    "    def get_num_batches(self,batch_size):\n",
    "        return len(self)//batch_size\n",
    "\n",
    "\n",
    "def generate_batches(dataset,batch_size,shuffle=True,drop_last=True,device='cpu'):\n",
    "    \"\"\"\n",
    "    数据集，batch_size,是否随机打乱数据，是否丢弃最后一个batch?,设备\n",
    "    \"\"\"\n",
    "    dataloader=DataLoader(dataset=dataset,batch_size=batch_size,shuffle=shuffle,drop_last=drop_last)\n",
    "    \n",
    "    for data_dict in dataloader:\n",
    "        out_data_dict = {}\n",
    "        for name, tensor in data_dict.items():\n",
    "            out_data_dict[name] = data_dict[name].to(device)\n",
    "        yield out_data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SurnameGenerationModel(nn.Module):\n",
    "    \n",
    "    def __init__(self,char_embedding_dim,char_vocab_size,rnn_hidden_dim,batch_first=True,padding_idx=0,dropout_p=0.5):\n",
    "        \n",
    "        super(SurnameGenerationModel,self).__init__()\n",
    "        \n",
    "        self.char_emb=nn.Embedding(num_embeddings=char_vocab_size,\n",
    "                                  embedding_dim=char_embedding_dim,\n",
    "                                  padding_idx=padding_idx)\n",
    "        \n",
    "        self.rnn=nn.GRU(input_size=char_embedding_dim,hidden_size=rnn_hidden_dim,batch_first=batch_first)\n",
    "        \n",
    "        self.fc=nn.Linear(rnn_hidden_dim,char_vocab_size)\n",
    "        \n",
    "        self._dropout_p=dropout_p\n",
    "    \n",
    "    def forward(self,x_in,apply_softmax=False):\n",
    "        \n",
    "        x_embedded=self.char_emb(x_in)\n",
    "        y_out,_=self.rnn(x_embedded)\n",
    "        \n",
    "        batch_size,seq_size,feat_size=y_out.shape\n",
    "        \n",
    "        #变成二维的\n",
    "        y_out=y_out.contiguous().view(batch_size*seq_size,feat_size)\n",
    "        \n",
    "        y_out=self.fc(F.dropout(y_out,p=self._dropout_p))\n",
    "        \n",
    "        if apply_softmax:\n",
    "            y_out=F.softmax(y_out,dim=1)\n",
    "        \n",
    "        new_feat_size=y_out.shape[-1]\n",
    "        y_out=y_out.view(batch_size,seq_size,new_feat_size)\n",
    "        #y_out 的shape 为batch_size  max_word_length   char_vocab_size\n",
    "        return y_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SurnameGenerationModel_conditioned(nn.Module):\n",
    "    \n",
    "    def __init__(self,char_embedding_dim,char_vocab_size,num_nationalities,rnn_hidden_dim,batch_first=True,padding_idx=0,dropout_p=0.5):\n",
    "        \n",
    "        super(SurnameGenerationModel_conditioned,self).__init__()\n",
    "        \n",
    "        self.char_emb=nn.Embedding(num_embeddings=char_vocab_size,\n",
    "                                  embedding_dim=char_embedding_dim,\n",
    "                                  padding_idx=padding_idx)\n",
    "        self.national_emb=nn.Embedding(num_embeddings=num_nationalities,\n",
    "                                            embedding_dim=rnn_hidden_dim)\n",
    "        self.rnn=nn.GRU(input_size=char_embedding_dim,hidden_size=rnn_hidden_dim,batch_first=batch_first)\n",
    "        \n",
    "        self.fc=nn.Linear(rnn_hidden_dim,char_vocab_size)\n",
    "        \n",
    "        self._dropout_p=dropout_p\n",
    "    \n",
    "    def forward(self,x_in,nationality_index,apply_softmax=False):\n",
    "        \n",
    "        x_embedded=self.char_emb(x_in)\n",
    "        nationality_embeded=self.national_emb(nationality_index).unsqueeze(0)\n",
    "        y_out,_=self.rnn(x_embedded,nationality_embeded)\n",
    "        \n",
    "        batch_size,seq_size,feat_size=y_out.shape\n",
    "        \n",
    "        #变成二维的\n",
    "        y_out=y_out.contiguous().view(batch_size*seq_size,feat_size)\n",
    "        \n",
    "        y_out=self.fc(F.dropout(y_out,p=self._dropout_p))\n",
    "        \n",
    "        if apply_softmax:\n",
    "            y_out=F.softmax(y_out,dim=1)\n",
    "        \n",
    "        new_feat_size=y_out.shape[-1]\n",
    "        y_out=y_out.view(batch_size,seq_size,new_feat_size)\n",
    "        #y_out 的shape 为batch_size  max_word_length   char_vocab_size\n",
    "        return y_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_from_model(model,vectorizer,num_samples=1,sample_size=20,temperature=1.0):\n",
    "    \"\"\"\n",
    "    使用训练好的模型\n",
    "    indices (torch.Tensor): the matrix of indices; \n",
    "        shape = (num_samples, sample_size)\n",
    "    \"\"\"\n",
    "    #获取和样本数目一样的begin_seq_index\n",
    "    begin_seq_index=[vectorizer.char_vocab.begin_seq_index for _ in range(num_samples)]\n",
    "    #转化为二维\n",
    "    begin_seq_index = torch.tensor(begin_seq_index, dtype=torch.int64).unsqueeze(dim=1)\n",
    "    #转化为3维\n",
    "    indices=[begin_seq_index]\n",
    "    \n",
    "    h_t=None\n",
    "    for time_step in range(sample_size):\n",
    "        #获取某个样本的\n",
    "        x_t=indices[time_step]\n",
    "        x_emb_t=model.char_emb(x_t)\n",
    "        rnn_out_t,h_t=model.rnn(x_emb_t,h_t)\n",
    "        #因为是char 所以数据的格式维度为 batch_size 1 feats_size\n",
    "        prediction_vector=model.fc(rnn_out_t.squeeze(dim=1))\n",
    "        \n",
    "        #使用softmax\n",
    "        probability_vector = F.softmax(prediction_vector / temperature, dim=1)\n",
    "        #取值，获取\n",
    "        indices.append(torch.multinomial(probability_vector, num_samples=1))\n",
    "        \n",
    "    indices = torch.stack(indices).squeeze().permute(1, 0)\n",
    "    return indices\n",
    "\n",
    "def decode_samples(sampled_indices,vectorizer):\n",
    "    \"\"\"将indices 转化为surname\n",
    "    \"\"\"\n",
    "    decoded_surnames=[]\n",
    "    vocab=vectorizer.char_vocab\n",
    "    \n",
    "    for sample_index in range(sampled_indices.shape[0]):\n",
    "        surname=''\n",
    "        for time_step in range(sampled_indices.shape[1]):\n",
    "            #获取每个时间步的index\n",
    "            sample_item=sampled_indices[sample_index,time_step].item()\n",
    "            #为开头就跳过\n",
    "            if sample_item == vocab.begin_seq_index:\n",
    "                continue\n",
    "            #为结尾就跳出\n",
    "            elif sample_item == vocab.end_seq_index:\n",
    "                break\n",
    "            else:\n",
    "                surname+=vocab.lookup_idx(sample_item)\n",
    "        decoded_surnames.append(surname)\n",
    "    return decoded_surnames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_train_state(args):\n",
    "    return {'stop_early': False,\n",
    "            'early_stopping_step': 0,\n",
    "            'early_stopping_best_val': 1e8,\n",
    "            'learning_rate': args.learning_rate,\n",
    "            'epoch_index': 0,\n",
    "            'train_loss': [],\n",
    "            'train_acc': [],\n",
    "            'val_loss': [],\n",
    "            'val_acc': [],\n",
    "            'test_loss': -1,\n",
    "            'test_acc': -1,\n",
    "            'model_filename': args.model_state_file}\n",
    "def update_train_state(args, model, train_state):\n",
    "    # Save one model at least\n",
    "    if train_state['epoch_index'] == 0:\n",
    "        torch.save(model.state_dict(), train_state['model_filename'])\n",
    "        train_state['stop_early'] = False\n",
    "\n",
    "    # Save model if performance improved\n",
    "    elif train_state['epoch_index'] >= 1:\n",
    "        loss_tm1, loss_t = train_state['val_loss'][-2:]\n",
    "         \n",
    "        # If loss worsened\n",
    "        if loss_t >= loss_tm1:\n",
    "            # Update step\n",
    "            train_state['early_stopping_step'] += 1\n",
    "        # Loss decreased\n",
    "        else:\n",
    "            # Save the best model\n",
    "            if loss_t < train_state['early_stopping_best_val']:\n",
    "                torch.save(model.state_dict(), train_state['model_filename'])\n",
    "                train_state['early_stopping_best_val'] = loss_t\n",
    "\n",
    "            # Reset early stopping step\n",
    "            train_state['early_stopping_step'] = 0\n",
    "\n",
    "        # Stop early ?\n",
    "        train_state['stop_early'] = \\\n",
    "            train_state['early_stopping_step'] >= args.early_stopping_criteria\n",
    "\n",
    "    return train_state\n",
    "\n",
    "def normalize_sizes(y_pred,y_true):\n",
    "    #y_pred 是模型的输出，如果是三维的那就转化为矩阵\n",
    "    #y_true是真实的值，为矩阵就转化为一维向量\n",
    "    if len(y_pred.size())==3:\n",
    "        y_pred=y_pred.contiguous().view(-1,y_pred.size(2))\n",
    "    if len(y_true.size()) == 2:\n",
    "        y_true = y_true.contiguous().view(-1)\n",
    "    return y_pred, y_true\n",
    "\n",
    "def compute_accuracy(y_pred, y_true, mask_index):\n",
    "    y_pred, y_true = normalize_sizes(y_pred, y_true)\n",
    "    _,y_pred_indices=y_pred.max(dim=1)\n",
    "    #根据真实的字符得出与模型输出的字符相比较     1 0 1 0 1 \n",
    "    correct_indices=torch.eq(y_pred_indices,y_true).float()\n",
    "    #两个tensor得出不相等的每项，相等为0，不想等为1      即真实值与mask  假设y_true 为 2 4 6 0 （0为mask_index）\n",
    "    #                                                                                  0 0 0 0\n",
    "    #则valil_indices 为                                                                1 1 1 0\n",
    "    valid_indices = torch.ne(y_true, mask_index).float()\n",
    "    #除去mask 后预测成功的值\n",
    "    n_correct = (correct_indices * valid_indices).sum().item()\n",
    "    #真实字符个数\n",
    "    n_valid = valid_indices.sum().item()\n",
    "    #准确率\n",
    "    return n_correct / n_valid * 100\n",
    "\n",
    "def sequence_loss(y_pred, y_true, mask_index):\n",
    "    \"\"\"\n",
    "    计算loss\n",
    "    \"\"\"\n",
    "    y_pred, y_true = normalize_sizes(y_pred, y_true)\n",
    "    return F.cross_entropy(y_pred, y_true, ignore_index=mask_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed_everywhere(seed, cuda):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if cuda:\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "def handle_dirs(dirpath):\n",
    "    if not os.path.exists(dirpath):\n",
    "        os.makedirs(dirpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Namespace(\n",
    "    #训练数据\n",
    "    surname_csv=\"./data/surnames/surnames_with_splits.csv\",\n",
    "    model_state_file=\"model.pth\",\n",
    "    save_dir=\"model_storage/model1_unconditioned_surname_generation\",\n",
    "    # Model hyper parameters\n",
    "    char_embedding_size=32,\n",
    "    rnn_hidden_size=32,\n",
    "    # Training hyper parameters\n",
    "    seed=1337,\n",
    "    learning_rate=0.001,\n",
    "    batch_size=128,\n",
    "    num_epochs=100,\n",
    "    early_stopping_criteria=5,\n",
    "    # Runtime options\n",
    "    catch_keyboard_interrupt=True,\n",
    "    cuda=True,\n",
    "    expand_filepaths_to_save_dir=True,\n",
    "    reload_from_files=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expanded filepaths: \n",
      "\tmodel_storage/model1_unconditioned_surname_generation\\model.pth\n"
     ]
    }
   ],
   "source": [
    "if args.expand_filepaths_to_save_dir:\n",
    "    args.model_state_file = os.path.join(args.save_dir,\n",
    "                                         args.model_state_file)\n",
    "    \n",
    "    print(\"Expanded filepaths: \")\n",
    "    print(\"\\t{}\".format(args.model_state_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CUDA: True\n"
     ]
    }
   ],
   "source": [
    "# Check CUDA\n",
    "if not torch.cuda.is_available():\n",
    "    args.cuda = False\n",
    "args.device = torch.device(\"cuda\" if args.cuda else \"cpu\")\n",
    "print(\"Using CUDA: {}\".format(args.cuda))\n",
    "\n",
    "# Set seed for reproducibility\n",
    "set_seed_everywhere(args.seed, args.cuda)\n",
    "\n",
    "# handle dirs\n",
    "handle_dirs(args.save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#加载数据集\n",
    "dataset = SurnameDataset.load_dataset_and_make_vectorizer(args.surname_csv)\n",
    "#获取向量化对象类\n",
    "vectorizer=dataset.get_vectorizer()\n",
    "\n",
    "model=SurnameGenerationModel_conditioned(char_embedding_dim=args.char_embedding_size,\n",
    "                            char_vocab_size=len(vectorizer.char_vocab),\n",
    "                            num_nationalities=len(vectorizer.nationality_vocab),\n",
    "                            rnn_hidden_dim=args.rnn_hidden_size,\n",
    "                            padding_idx=vectorizer.char_vocab.mask_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e11048787954f12b8cf680a595d63f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='training routine', style=ProgressStyle(description_width='ini…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "639fcdb6f4f54746814bc8e1a666c3bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='split=train', max=60, style=ProgressStyle(description_width='…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3dea2e18168c4b0a86529ab416fd26d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='split=val', max=12, style=ProgressStyle(description_width='in…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mask_index=vectorizer.char_vocab.mask_index\n",
    "\n",
    "model=model.to(args.device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=args.learning_rate)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer,\n",
    "                                           mode='min', factor=0.5,\n",
    "                                           patience=1)\n",
    "train_state = make_train_state(args)\n",
    "\n",
    "epoch_bar = tqdm_notebook(desc='training routine', \n",
    "                          total=args.num_epochs,\n",
    "                          position=0)\n",
    "\n",
    "dataset.set_split('train')\n",
    "train_bar = tqdm_notebook(desc='split=train',\n",
    "                          total=dataset.get_num_batches(args.batch_size), \n",
    "                          position=1, \n",
    "                          leave=True)\n",
    "dataset.set_split('val')\n",
    "val_bar = tqdm_notebook(desc='split=val',\n",
    "                        total=dataset.get_num_batches(args.batch_size), \n",
    "                        position=1, \n",
    "                        leave=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    \n",
    "    for epoch_index in range(args.num_epochs):\n",
    "        train_state['epoch_index']=epoch_index\n",
    "        \n",
    "        dataset.set_split('train')\n",
    "        batch_generator = generate_batches(dataset, \n",
    "                                           batch_size=args.batch_size, \n",
    "                                           device=args.device)\n",
    "        running_loss = 0.0\n",
    "        running_acc = 0.0\n",
    "        model.train()\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        for batch_index, batch_dict in enumerate(batch_generator):\n",
    "            \n",
    "            #梯度归零\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            #得到模型输出\n",
    "            y_pred = model(x_in=batch_dict['x_data'],nationality_index=batch_dict['class_index'])\n",
    "            \n",
    "            #计算loss\n",
    "            loss = sequence_loss(y_pred, batch_dict['y_target'], mask_index)\n",
    "            \n",
    "            #梯度下降\n",
    "            loss.backward()\n",
    "            \n",
    "            #参数更新\n",
    "            optimizer.step()\n",
    "            \n",
    "            #计算loss 和准确度\n",
    "            running_loss += (loss.item() - running_loss) / (batch_index + 1)\n",
    "            acc_t = compute_accuracy(y_pred, batch_dict['y_target'], mask_index)\n",
    "            running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
    "            \n",
    "            #更新进度条\n",
    "            train_bar.set_postfix(loss=running_loss,\n",
    "                                  acc=running_acc,\n",
    "                                  epoch=epoch_index)\n",
    "            train_bar.update()\n",
    "        \n",
    "        train_state['train_loss'].append(running_loss)\n",
    "        train_state['train_acc'].append(running_acc)\n",
    "        \n",
    "        \n",
    "        \n",
    "        dataset.set_split('val')\n",
    "        batch_generator = generate_batches(dataset, \n",
    "                                           batch_size=args.batch_size, \n",
    "                                           device=args.device)\n",
    "        running_loss = 0.\n",
    "        running_acc = 0.\n",
    "        model.eval()\n",
    "        \n",
    "        \n",
    "        for batch_index, batch_dict in enumerate(batch_generator):\n",
    "            # compute the output\n",
    "            y_pred = model(x_in=batch_dict['x_data'],nationality_index=batch_dict['class_index'])\n",
    "\n",
    "            # step 3. compute the loss\n",
    "            loss = sequence_loss(y_pred, batch_dict['y_target'], mask_index)\n",
    "\n",
    "            # compute the  running loss and running accuracy\n",
    "            running_loss += (loss.item() - running_loss) / (batch_index + 1)\n",
    "            acc_t = compute_accuracy(y_pred, batch_dict['y_target'], mask_index)\n",
    "            running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
    "            \n",
    "            # Update bar\n",
    "            val_bar.set_postfix(loss=running_loss, acc=running_acc, \n",
    "                            epoch=epoch_index)\n",
    "            val_bar.update()\n",
    "\n",
    "        train_state['val_loss'].append(running_loss)\n",
    "        train_state['val_acc'].append(running_acc)\n",
    "\n",
    "        train_state = update_train_state(args=args, model=model, \n",
    "                                         train_state=train_state)\n",
    "\n",
    "        scheduler.step(train_state['val_loss'][-1])\n",
    "        \n",
    "        \n",
    "        if train_state['stop_early']:\n",
    "            break\n",
    "        \n",
    "        model=model.cpu()\n",
    "        sampled_surname=decode_samples(sample_from_model(model,vectorizer,num_samples=2),vectorizer)\n",
    "        epoch_bar.set_postfix(sample1=sampled_surname[0], sample2=sampled_surname[1])\n",
    "        \n",
    "        \n",
    "        model = model.to(args.device)\n",
    "        \n",
    "        train_bar.n = 0\n",
    "        val_bar.n = 0\n",
    "        epoch_bar.update()\n",
    "        \n",
    "except KeyboardInterrupt:\n",
    "    print(\"Exiting loop\")        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#加载效果最好的模型\n",
    "model.load_state_dict(torch.load(train_state['model_filename']))\n",
    "#转化为\n",
    "model = model.to(args.device)\n",
    "\n",
    "dataset.set_split('test')\n",
    "batch_generator = generate_batches(dataset, \n",
    "                                   batch_size=args.batch_size, \n",
    "                                   device=args.device)\n",
    "running_acc = 0.\n",
    "model.eval()\n",
    "\n",
    "for batch_index, batch_dict in enumerate(batch_generator):\n",
    "    # compute the output\n",
    "    y_pred = model(x_in=batch_dict['x_data'],nationality_index=batch_dict['class_index'])\n",
    "\n",
    "    # compute the loss\n",
    "    loss = sequence_loss(y_pred, batch_dict['y_target'], mask_index)\n",
    "\n",
    "    # compute the accuracy\n",
    "    running_loss += (loss.item() - running_loss) / (batch_index + 1)\n",
    "\n",
    "    acc_t = compute_accuracy(y_pred, batch_dict['y_target'], mask_index)\n",
    "    running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
    "\n",
    "train_state['test_loss'] = running_loss \n",
    "train_state['test_acc'] = running_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 2.455001731713613;\n",
      "Test Accuracy: 28.17397030709523\n"
     ]
    }
   ],
   "source": [
    "print(\"Test loss: {};\".format(train_state['test_loss']))\n",
    "print(\"Test Accuracy: {}\".format(train_state['test_acc']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------\n",
      "Mirtilab\n",
      "Lae\n",
      "Frarrotoem\n",
      "Mitos\n",
      "Lavoon\n",
      "Zhekcil\n",
      "Aor\n",
      "Chenmalascke\n",
      "Janv\n",
      "Arrcan\n"
     ]
    }
   ],
   "source": [
    "#测试，随机生成10各样本\n",
    "num_names = 10\n",
    "model = model.cpu()\n",
    "# Generate nationality hidden state\n",
    "sampled_surnames = decode_samples(\n",
    "    sample_from_model(model, vectorizer, num_samples=num_names), \n",
    "    vectorizer)\n",
    "# Show results\n",
    "print (\"-\"*15)\n",
    "for i in range(num_names):\n",
    "    print (sampled_surnames[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
