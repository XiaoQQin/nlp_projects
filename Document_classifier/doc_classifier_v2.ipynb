{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "from tqdm._tqdm_notebook import tqdm_notebook as tqdm\n",
    "from keras.preprocessing import text, sequence\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils import data\n",
    "from torch.nn import functional as F\n",
    "from argparse import Namespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv('./data/news_with_splits.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>split</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Business</td>\n",
       "      <td>train</td>\n",
       "      <td>jobs , tax cuts key issues for bush</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Business</td>\n",
       "      <td>train</td>\n",
       "      <td>jarden buying mr . coffee s maker</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Business</td>\n",
       "      <td>train</td>\n",
       "      <td>retail sales show festive fervour</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Business</td>\n",
       "      <td>train</td>\n",
       "      <td>intervoice s customers come calling</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Business</td>\n",
       "      <td>train</td>\n",
       "      <td>boeing expects air force contract</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   category  split                                title\n",
       "0  Business  train  jobs , tax cuts key issues for bush\n",
       "1  Business  train    jarden buying mr . coffee s maker\n",
       "2  Business  train    retail sales show festive fervour\n",
       "3  Business  train  intervoice s customers come calling\n",
       "4  Business  train    boeing expects air force contract"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Business', 'Sci/Tech', 'Sports', 'World'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categories=set(data.category)\n",
    "categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "results={}\n",
    "for i,value in  enumerate(categories):\n",
    "    results[value]=i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'World': 0, 'Sports': 1, 'Sci/Tech': 2, 'Business': 3}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['category']=data['category'].map(results).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>split</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>train</td>\n",
       "      <td>jobs , tax cuts key issues for bush</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>train</td>\n",
       "      <td>jarden buying mr . coffee s maker</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>train</td>\n",
       "      <td>retail sales show festive fervour</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>train</td>\n",
       "      <td>intervoice s customers come calling</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>train</td>\n",
       "      <td>boeing expects air force contract</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   category  split                                title\n",
       "0         3  train  jobs , tax cuts key issues for bush\n",
       "1         3  train    jarden buying mr . coffee s maker\n",
       "2         3  train    retail sales show festive fervour\n",
       "3         3  train  intervoice s customers come calling\n",
       "4         3  train    boeing expects air force contract"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "120000"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NewsDataset(Dataset):\n",
    "    \n",
    "    def __init__(self,tokenizer,data):\n",
    "        \n",
    "        self.tokenizer=tokenizer\n",
    "        self.data=data\n",
    "        \n",
    "        #获取文本的最长长度\n",
    "        measure_len=lambda context:len(context.split(' '))\n",
    "        self.max_seq_length=max(map(measure_len,self.data.title))\n",
    "        \n",
    "        \n",
    "        #获取各种类型数据的数据集\n",
    "        train_data=data[data.split=='train']\n",
    "        val_data=data[data.split=='val']\n",
    "        test_data=data[data.split=='test']\n",
    "        \n",
    "        #获取各个样本的x转化为index,\n",
    "        x_train = tokenizer.texts_to_sequences(list(train_data.title))\n",
    "        x_val=tokenizer.texts_to_sequences(list(val_data.title))\n",
    "        x_test = tokenizer.texts_to_sequences(list(test_data.title))\n",
    "        \n",
    "        #pad,即获取各数据集的x_data\n",
    "        self.x_train = sequence.pad_sequences(x_train, maxlen=self.max_seq_length)\n",
    "        self.x_val=sequence.pad_sequences(x_val,maxlen=self.max_seq_length)\n",
    "        self.x_test = sequence.pad_sequences(x_test, maxlen=self.max_seq_length)\n",
    "        \n",
    "        #获取y值\n",
    "        self.y_train=list(train_data.category)\n",
    "        self.y_val=list(val_data.category)\n",
    "        self.y_test=list(test_data.category)\n",
    "        \n",
    "        #获取各样本的长度\n",
    "        self.train_size=len(train_data)\n",
    "        self.val_size=len(val_data)\n",
    "        self.test_size=len(test_data)\n",
    "        \n",
    "        #建立数据集字典\n",
    "        self.lookup_dict={\n",
    "            'train':(self.x_train,self.y_train,self.train_size),\n",
    "            'val':(self.x_val,self.y_val,self.val_size),\n",
    "            'test':(self.x_test,self.y_test,self.test_size)\n",
    "        }\n",
    "        \n",
    "        #默认训练数据\n",
    "        self.set_split('train')\n",
    "    \n",
    "    #设置数据集\n",
    "    def set_split(self,split='train'):\n",
    "        self.target_x,self.target_y,self.target_size=self.lookup_dict[split]\n",
    "        \n",
    "    #计算多少个batch\n",
    "    def get_num_batches(self,batch_size):\n",
    "        return len(self)//batch_size\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.target_size\n",
    "    \n",
    "    def __getitem__(self,index):\n",
    "        x_data=self.target_x[index]\n",
    "        y_data=self.target_y[index]\n",
    "        \n",
    "        x_data=torch.tensor(x_data,dtype=torch.long)\n",
    "        y_data=torch.tensor(y_data,dtype=torch.long)\n",
    "        return {\n",
    "            'x_data':x_data,\n",
    "            'y_target':y_data\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NewsClassifier(nn.Module):\n",
    "    def __init__(self,embedding_matrix,max_features,num_channels,hidden_dim,num_classes,dropout_p):\n",
    "        super(NewsClassifier,self).__init__()\n",
    "        embed_size = embedding_matrix.shape[1]\n",
    "        self.embedding = nn.Embedding(max_features, embed_size)\n",
    "        self.embedding.weight = nn.Parameter(torch.tensor(embedding_matrix, dtype=torch.float32))\n",
    "        self.embedding.weight.requires_grad = False\n",
    "        self.conv=nn.Sequential(\n",
    "            nn.Conv1d(embed_size,num_channels,3),\n",
    "            nn.ELU(),\n",
    "            nn.Conv1d(num_channels,num_channels,3,stride=2),\n",
    "            nn.ELU(),\n",
    "            nn.Conv1d(num_channels,num_channels,3,stride=2),\n",
    "            nn.ELU(),\n",
    "            nn.Conv1d(num_channels,num_channels,3),\n",
    "            nn.ELU()\n",
    "        )\n",
    "        self.dropout_p=dropout_p\n",
    "        \n",
    "        self.fc1=nn.Linear(num_channels,hidden_dim)\n",
    "        self.fc2=nn.Linear(hidden_dim,num_classes)\n",
    "        \n",
    "    def  forward(self,x_in,apply_softmax=False):\n",
    "        \n",
    "        #permute 变化size\n",
    "        x_embedded=self.embedding(x_in).permute(0,2,1)\n",
    "        features=self.conv(x_embedded)\n",
    "        \n",
    "        remaining_size=features.size(dim=2)\n",
    "        features=F.avg_pool1d(features,remaining_size).squeeze(dim=2)\n",
    "        features = F.dropout(features, p=self.dropout_p)\n",
    "        \n",
    "        intermediate_vector = F.relu(F.dropout(self.fc1(features), p=self.dropout_p))\n",
    "        prediction_vector = self.fc2(intermediate_vector)\n",
    "        \n",
    "        if apply_softmax:\n",
    "            prediction_vector = F.softmax(prediction_vector, dim=1)\n",
    "        return prediction_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer=text.Tokenizer()\n",
    "tokenizer.fit_on_texts(list(data.title))\n",
    "newsDataset=NewsDataset(tokenizer,data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.LongTensor\n",
      "torch.LongTensor\n"
     ]
    }
   ],
   "source": [
    "dataloader=DataLoader(dataset=newsDataset,batch_size=128,shuffle=True,drop_last=True)\n",
    "for data_dict in dataloader:\n",
    "    print(data_dict['x_data'].type())\n",
    "    print(data_dict['y_target'].type())\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#用来生成数据\n",
    "def generate_batches(dataset,batch_size,shuffle=True,drop_last=True,device='cpu'):\n",
    "        dataloader=DataLoader(dataset=dataset,batch_size=batch_size,shuffle=shuffle,drop_last=drop_last)\n",
    "        \n",
    "        for data_dict in dataloader:\n",
    "            out_data_dict={}\n",
    "            \n",
    "            for name,tensor in data_dict.items():\n",
    "                out_data_dict[name]=data_dict[name].to(device)\n",
    "            yield out_data_dict\n",
    "#创作训练状态\n",
    "def make_train_state(args):\n",
    "    return {'stop_early': False,\n",
    "            'early_stopping_step': 0,\n",
    "            'early_stopping_best_val': 1e8,\n",
    "            'learning_rate': args.learning_rate,\n",
    "            'epoch_index': 0,\n",
    "            'train_loss': [],\n",
    "            'train_acc': [],\n",
    "            'val_loss': [],\n",
    "            'val_acc': [],\n",
    "            'test_loss': -1,\n",
    "            'test_acc': -1,\n",
    "            'model_filename': args.model_state_file}\n",
    "#更新训练状态\n",
    "def update_train_state(args, model, train_state):\n",
    "    \n",
    "    # Save one model at least\n",
    "    if train_state['epoch_index'] == 0:\n",
    "        torch.save(model.state_dict(), train_state['model_filename'])\n",
    "        train_state['stop_early'] = False\n",
    "\n",
    "    # Save model if performance improved\n",
    "    elif train_state['epoch_index'] >= 1:\n",
    "        loss_tm1, loss_t = train_state['val_loss'][-2:]\n",
    "\n",
    "        # If loss worsened\n",
    "        if loss_t >= train_state['early_stopping_best_val']:\n",
    "            # Update step\n",
    "            train_state['early_stopping_step'] += 1\n",
    "        # Loss decreased\n",
    "        else:\n",
    "            # Save the best model\n",
    "            if loss_t < train_state['early_stopping_best_val']:\n",
    "                torch.save(model.state_dict(), train_state['model_filename'])\n",
    "\n",
    "            # Reset early stopping step\n",
    "            train_state['early_stopping_step'] = 0\n",
    "\n",
    "        # Stop early ?\n",
    "        train_state['stop_early'] = train_state['early_stopping_step'] >= args.early_stopping_criteria\n",
    "\n",
    "    return train_state\n",
    "#计算准确率\n",
    "def compute_accuracy(y_pred, y_target):\n",
    "    _, y_pred_indices = y_pred.max(dim=1)\n",
    "    n_correct = torch.eq(y_pred_indices, y_target).sum().item()\n",
    "    return n_correct / len(y_pred_indices) * 100\n",
    "\n",
    "#\n",
    "def set_seed_everywhere(seed, cuda):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if cuda:\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "#文件夹处理\n",
    "def handle_dirs(dirpath):\n",
    "    if not os.path.exists(dirpath):\n",
    "        os.makedirs(dirpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_coefs(word, *arr):\n",
    "    return word, np.asarray(arr, dtype='float32')\n",
    "#加载普通txt和vec预训练词向量\n",
    "def load_embeddings_normal(path):\n",
    "    with open(path,'r',encoding='utf-8') as f:\n",
    "        return dict(get_coefs(*line.strip().split(' ')) for line in tqdm(f))\n",
    "    \n",
    "#建议英文文本时，不要将所有文本转化为小写，这可能丢失许多信息\n",
    "#根据word_index 建立index_word矩阵，如果当前word在词中找不到对应的向量，转化为小写\n",
    "def build_matrix(word_index, path,embedding_size):\n",
    "    embedding_index = load_embeddings_normal(path)\n",
    "    embedding_matrix = np.zeros((max_features + 1, embedding_size))\n",
    "    unknown_words = []\n",
    "    \n",
    "    for word, i in word_index.items():\n",
    "        if i <= max_features:\n",
    "            try:\n",
    "                embedding_matrix[i] = embedding_index[word]\n",
    "            except KeyError:\n",
    "                try:\n",
    "                    embedding_matrix[i] = embedding_index[word.lower()]\n",
    "                except KeyError:\n",
    "                    try:\n",
    "                        embedding_matrix[i] = embedding_index[word.title()]\n",
    "                    except KeyError:\n",
    "                        unknown_words.append(word)\n",
    "    return embedding_matrix, unknown_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expanded filepaths: \n",
      "\tmodel_storage/document_classification\\model.pth\n",
      "Using CUDA: True\n"
     ]
    }
   ],
   "source": [
    "#训练参数\n",
    "args = Namespace(\n",
    "    # Data and Path hyper parameters\n",
    "    news_csv=\"data/news_with_splits.csv\",\n",
    "    model_state_file=\"model.pth\",\n",
    "    save_dir=\"model_storage/document_classification\",\n",
    "    # Model hyper parameters\n",
    "    glove_filepath='data/glove/glove.6B.100d.txt', \n",
    "    embedding_size=100, \n",
    "    hidden_dim=100, \n",
    "    num_channels=100, \n",
    "    # Training hyper parameter\n",
    "    seed=1337, \n",
    "    learning_rate=0.001, \n",
    "    dropout_p=0.1, \n",
    "    batch_size=128, \n",
    "    num_epochs=100, \n",
    "    early_stopping_criteria=5, \n",
    "    # Runtime option\n",
    "    cuda=True, \n",
    "    expand_filepaths_to_save_dir=True\n",
    ") \n",
    "\n",
    "#如果选择创建文件夹\n",
    "if args.expand_filepaths_to_save_dir:\n",
    "   \n",
    "    args.model_state_file = os.path.join(args.save_dir,\n",
    "                                         args.model_state_file)    \n",
    "    print(\"Expanded filepaths: \")\n",
    "    print(\"\\t{}\".format(args.model_state_file))\n",
    "# 检查CUDA\n",
    "if not torch.cuda.is_available():\n",
    "    args.cuda = False\n",
    "    \n",
    "args.device = torch.device(\"cuda\" if args.cuda else \"cpu\")\n",
    "print(\"Using CUDA: {}\".format(args.cuda))\n",
    "\n",
    "# Set seed for reproducibility\n",
    "set_seed_everywhere(args.seed, args.cuda)\n",
    "\n",
    "# handle dirs\n",
    "handle_dirs(args.save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = None\n",
    "max_features = max_features or len(tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "202f63ec030e469fbef7130781cb4296",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "n unknown words (crawl):  1682\n"
     ]
    }
   ],
   "source": [
    "#建立词向量矩阵\n",
    "glove_matrix, unknown_words_glove = build_matrix(tokenizer.word_index, args.glove_filepath,args.embedding_size)\n",
    "print('n unknown words (crawl): ', len(unknown_words_glove))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32183"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#创建模型\n",
    "classifier=NewsClassifier(embedding_matrix=glove_matrix,max_features=max_features,num_channels=args.num_channels,\n",
    "                         hidden_dim=args.hidden_dim,\n",
    "                         num_classes=4,\n",
    "                         dropout_p=args.dropout_p\n",
    "                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NewsClassifier(\n",
      "  (embedding): Embedding(32183, 100)\n",
      "  (conv): Sequential(\n",
      "    (0): Conv1d(100, 100, kernel_size=(3,), stride=(1,))\n",
      "    (1): ELU(alpha=1.0)\n",
      "    (2): Conv1d(100, 100, kernel_size=(3,), stride=(2,))\n",
      "    (3): ELU(alpha=1.0)\n",
      "    (4): Conv1d(100, 100, kernel_size=(3,), stride=(2,))\n",
      "    (5): ELU(alpha=1.0)\n",
      "    (6): Conv1d(100, 100, kernel_size=(3,), stride=(1,))\n",
      "    (7): ELU(alpha=1.0)\n",
      "  )\n",
      "  (fc1): Linear(in_features=100, out_features=100, bias=True)\n",
      "  (fc2): Linear(in_features=100, out_features=4, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#查看模型结构\n",
    "print(classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "985dfc066b094fb79100e9bafe959dd3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='training routine', style=ProgressStyle(description_width='ini…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "104991d7c8a643fab06a7195fe03e4fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='split=train', max=656, style=ProgressStyle(description_width=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b4bb8b367b742bd8b0ea71b2494a6a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='split=val', max=140, style=ProgressStyle(description_width='i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "print(args.device)\n",
    "classifier=classifier.to(args.device)\n",
    "loss_func=nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(classifier.parameters(), lr=args.learning_rate)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer,\n",
    "                                           mode='min', factor=0.5,\n",
    "                                           patience=1)\n",
    "\n",
    "train_state = make_train_state(args)\n",
    "epoch_bar = tqdm(desc='training routine', \n",
    "                          total=args.num_epochs,\n",
    "                          position=0)\n",
    "\n",
    "newsDataset.set_split('train')\n",
    "train_bar = tqdm(desc='split=train',\n",
    "                          total=newsDataset.get_num_batches(args.batch_size), \n",
    "                          position=1, \n",
    "                          leave=True)\n",
    "newsDataset.set_split('val')\n",
    "val_bar = tqdm(desc='split=val',\n",
    "                        total=newsDataset.get_num_batches(args.batch_size), \n",
    "                        position=1, \n",
    "                        leave=True)\n",
    "try:\n",
    "    for epoch_index in range(args.num_epochs):\n",
    "        train_state['epoch_index'] = epoch_index\n",
    "        \n",
    "        newsDataset.set_split('train')\n",
    "        batch_generator = generate_batches(newsDataset, \n",
    "                                           batch_size=args.batch_size, \n",
    "                                           device=args.device)\n",
    "        running_loss = 0.0\n",
    "        running_acc = 0.0\n",
    "        classifier.train()\n",
    "        \n",
    "        \n",
    "        for batch_index, batch_dict in enumerate(batch_generator):\n",
    "            # the training routine is these 5 steps:\n",
    "\n",
    "            # --------------------------------------\n",
    "            # step 1. zero the gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # step 2. compute the output\n",
    "            y_pred = classifier(batch_dict['x_data'])\n",
    "\n",
    "            # step 3. compute the loss\n",
    "            loss = loss_func(y_pred, batch_dict['y_target'])\n",
    "            loss_t = loss.item()\n",
    "            running_loss += (loss_t - running_loss) / (batch_index + 1)\n",
    "\n",
    "            # step 4. use loss to produce gradients\n",
    "            loss.backward()\n",
    "\n",
    "            # step 5. use optimizer to take gradient step\n",
    "            optimizer.step()\n",
    "            # -----------------------------------------\n",
    "            # compute the accuracy\n",
    "            acc_t = compute_accuracy(y_pred, batch_dict['y_target'])\n",
    "            running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
    "\n",
    "            # update bar\n",
    "            train_bar.set_postfix(loss=running_loss, acc=running_acc, \n",
    "                                  epoch=epoch_index)\n",
    "            train_bar.update()\n",
    "\n",
    "        train_state['train_loss'].append(running_loss)\n",
    "        train_state['train_acc'].append(running_acc)\n",
    "        # Iterate over val dataset\n",
    "\n",
    "        # setup: batch generator, set loss and acc to 0; set eval mode on\n",
    "        newsDataset.set_split('val')\n",
    "        batch_generator = generate_batches(newsDataset, \n",
    "                                           batch_size=args.batch_size, \n",
    "                                           device=args.device)\n",
    "        running_loss = 0.\n",
    "        running_acc = 0.\n",
    "        classifier.eval()\n",
    "\n",
    "        for batch_index, batch_dict in enumerate(batch_generator):\n",
    "            # compute the output\n",
    "            y_pred =  classifier(batch_dict['x_data'])\n",
    "\n",
    "            # step 3. compute the loss\n",
    "            loss = loss_func(y_pred, batch_dict['y_target'])\n",
    "            loss_t = loss.item()\n",
    "            running_loss += (loss_t - running_loss) / (batch_index + 1)\n",
    "\n",
    "            # compute the accuracy\n",
    "            acc_t = compute_accuracy(y_pred, batch_dict['y_target'])\n",
    "            running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
    "            val_bar.set_postfix(loss=running_loss, acc=running_acc, \n",
    "                            epoch=epoch_index)\n",
    "            val_bar.update()\n",
    "\n",
    "        train_state['val_loss'].append(running_loss)\n",
    "        train_state['val_acc'].append(running_acc)\n",
    "\n",
    "        train_state = update_train_state(args=args, model=classifier,\n",
    "                                         train_state=train_state)\n",
    "\n",
    "        scheduler.step(train_state['val_loss'][-1])\n",
    "\n",
    "        if train_state['stop_early']:\n",
    "            break\n",
    "\n",
    "        train_bar.n = 0\n",
    "        val_bar.n = 0\n",
    "        epoch_bar.update()\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Exiting loop\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
