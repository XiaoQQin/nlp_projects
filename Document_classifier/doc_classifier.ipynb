{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from argparse import Namespace\n",
    "from collections import Counter\n",
    "import json\n",
    "import re\n",
    "import string\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary(object):\n",
    "    #初始化\n",
    "    def __init__(self,token_to_idx=None):\n",
    "        if token_to_idx is None:\n",
    "            token_to_idx={}\n",
    "        self.token_to_idx=token_to_idx\n",
    "        self.idx_to_token={v:k for k,v in self.token_to_idx.items()}\n",
    "    #序列化输出\n",
    "    def to_serializable(self):\n",
    "        return {'token_to_idx':self.token_to_idx}\n",
    "    @classmethod\n",
    "    def from_serializable(cls,contents):\n",
    "        return cls(**contents)\n",
    "    #添加token\n",
    "    def add_token(self,token):\n",
    "        if token in self.token_to_idx:\n",
    "            return self.token_to_idx[token]\n",
    "        else:\n",
    "            index=len(self.token_to_idx)\n",
    "            self.token_to_idx[token]=index\n",
    "            self.idx_to_token[index]=token\n",
    "        \n",
    "        return index\n",
    "    #添加多个token\n",
    "    def add_many(self,tokens):\n",
    "        return [self.add_token(token) for token in tokens]\n",
    "    #查找token的idx\n",
    "    def lookup_token(self,token):\n",
    "        return self.token_to_idx[token]\n",
    "    #查找idx的token\n",
    "    def lookup_idx(self,index):\n",
    "        if index not in self.idx_to_idx:\n",
    "            raise KeyError(\"the index (%d) is not in the Vocabulary\" % index)\n",
    "        return self.idx_to_token(index)\n",
    "    #\n",
    "    def __str__(self):\n",
    "        return \"<Vocabulary(size=%d)>\" % len(self)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.token_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequenceVocabulary(Vocabulary):\n",
    "    \n",
    "    def __init__(self,token_to_idx=None,unk_token='<UNK>',mask_token='<MASK>',begin_seq_token='<BEGIN>',end_seq_token='<END>'):\n",
    "        super(SequenceVocabulary,self).__init__(token_to_idx)\n",
    "        \n",
    "        self.mask_token=mask_token\n",
    "        self.unk_token=unk_token\n",
    "        self.begin_seq_token=begin_seq_token\n",
    "        self.end_seq_token=end_seq_token\n",
    "        \n",
    "        self.mask_idx=self.add_token(self.mask_token)\n",
    "        self.unk_idx=self.add_token(self.unk_token)\n",
    "        self.begin_seq_idx=self.add_token(self.begin_seq_token)\n",
    "        self.end_seq_idx=self.add_token(self.end_seq_token)\n",
    "    \n",
    "    #序列化\n",
    "    def to_serializable(self):\n",
    "        #获取父类的序列化输出\n",
    "        contents=super(SequenceVocabulary,self).to_serializable()\n",
    "        #加入子类的数据\n",
    "        contents.update(\n",
    "        {\n",
    "            'unk_token':self.unk_token,\n",
    "            'mask_token':self.mask_token,\n",
    "            'begin_seq_token':self.begin_seq_token,\n",
    "            'end_seq_token':self.end_seq_token\n",
    "        })\n",
    "    def lookup_token(self,token):\n",
    "        if self.unk_idx>=0:\n",
    "            return self.token_to_idx.get(token,self.unk_idx)\n",
    "        else:\n",
    "            return self.token_to_idx[token]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NewsVectorizer(object):\n",
    "    \n",
    "    def __init__(self,title_vocab,category_vocab):\n",
    "        #文本字典\n",
    "        self.title_vocab=title_vocab\n",
    "        #标签字典\n",
    "        self.category_vocab=category_vocab\n",
    "    \n",
    "    \n",
    "    def vectorize(self,title,vector_length=-1):\n",
    "        \n",
    "        indices=[self.title_vocab.begin_seq_idx]\n",
    "        #找出句子中的每个token的序号\n",
    "        indices.extend(self.title_vocab.lookup_token(token) for token in title.split(' '))\n",
    "        indices.append(self.title_vocab.end_seq_idx)\n",
    "        \n",
    "        if vector_length<0:\n",
    "            vector_length=len(indices)\n",
    "        #vector_length 为超参数，即为训练文本的长度\n",
    "        out_vector = np.zeros(vector_length, dtype=np.int64)\n",
    "        out_vector[:len(indices)]=indices\n",
    "        out_vector[len(indices):]=self.title_vocab.mask_idx\n",
    "        \n",
    "        \n",
    "        return out_vector\n",
    "    \n",
    "    \n",
    "    @classmethod\n",
    "    def from_dataframe(cls, news_df, cutoff=25):\n",
    "        \"\"\"\n",
    "        从csv文件加载\n",
    "        \"\"\"\n",
    "        category_vocab = Vocabulary() \n",
    "        #创建标签字典\n",
    "        for category in sorted(set(news_df.category)):\n",
    "            category_vocab.add_token(category)\n",
    "        \n",
    "        #统计每个token 出现的次数\n",
    "        word_counts=Counter()\n",
    "        for title in news_df.title:\n",
    "            for token in title.split(' '):\n",
    "                if token not in string.punctuation:\n",
    "                    word_counts[token]+=1\n",
    "        \n",
    "        #文本字典\n",
    "        title_vocab=SequenceVocabulary()\n",
    "        for word,word_count in word_counts.items():\n",
    "            #如果token 出现的频率大于cutoff，就添加入词典\n",
    "            if word_count>cutoff==25:\n",
    "                title_vocab.add_token(word)\n",
    "        \n",
    "        return cls(title_vocab,category_vocab)\n",
    "    #根据序列化初始\n",
    "    @classmethod\n",
    "    def from_serializable(cls, contents):\n",
    "        title_vocab =SequenceVocabulary.from_serializable(contents['title_vocab'])\n",
    "        category_vocab =Vocabulary.from_serializable(contents['category_vocab'])\n",
    "\n",
    "        return cls(title_vocab=title_vocab, category_vocab=category_vocab)\n",
    "    #转化为序列化\n",
    "    def to_serializable(self):\n",
    "        return {'title_vocab': self.title_vocab.to_serializable(),\n",
    "                'category_vocab': self.category_vocab.to_serializable()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#创建数据集的类\n",
    "class NewsDataset(Dataset):\n",
    "    \n",
    "    def __init__(self,news_df,vectorizer):\n",
    "        self.news_df=news_df\n",
    "        self.vectorizer=vectorizer\n",
    "        \n",
    "        measure_len=lambda context:len(context.split(' '))\n",
    "        #获取文本的最长长度\n",
    "        self.max_seq_length=max(map(measure_len,news_df.title))+2\n",
    "        #获取各数据集\n",
    "        self.train_df = self.news_df[self.news_df.split=='train']\n",
    "        self.train_size = len(self.train_df)\n",
    "\n",
    "        self.val_df = self.news_df[self.news_df.split=='val']\n",
    "        self.val_size = len(self.val_df)\n",
    "\n",
    "        self.test_df = self.news_df[self.news_df.split=='test']\n",
    "        self.test_size = len(self.test_df)\n",
    "        \n",
    "        #建立数据集字典\n",
    "        self.lookup_dict={\n",
    "            'train':(self.train_df,self.train_size),\n",
    "            'val':(self.val_df,self.val_size),\n",
    "            'test':(self.test_df,self.test_size)\n",
    "        }\n",
    "        \n",
    "        #设置初始化哪种数据\n",
    "        self.set_split('train')\n",
    "        \n",
    "        \n",
    "        #统计各标签样本的多少\n",
    "        class_counts=news_df.category.value_counts().to_dict()\n",
    "        \n",
    "        def sort_key(item):\n",
    "            #返回标签的序号\n",
    "            return self.vectorizer.category_vocab.lookup_token(item[0])\n",
    "        #根据标签序号进行排序\n",
    "        sorted_counts = sorted(class_counts.items(), key=sort_key)\n",
    "        #获取每个标签的数据集大小\n",
    "        frequencies=[count for _,count in sorted_counts]\n",
    "        \n",
    "        self.class_weights=1.0/torch.tensor(frequencies,dtype=torch.float32)\n",
    "    \n",
    "    #加载数据并且创造Vectorizer类\n",
    "    @classmethod\n",
    "    def load_dataset_and_make_vectorizer(cls, news_csv):\n",
    "        news_df=pd.read_csv(news_csv)\n",
    "        train_news_df=news_df[news_df.split=='train']\n",
    "        \n",
    "        return cls(news_df,NewsVectorizer.from_dataframe(train_news_df))\n",
    "    \n",
    "    #加载数据和vectorizer\n",
    "    @classmethod\n",
    "    def load_dataset_and_load_vectorizer(cls, news_csv, vectorizer_filepath):\n",
    "        news_df = pd.read_csv(news_csv)\n",
    "        vectorizer = cls.load_vectorizer_only(vectorizer_filepath)\n",
    "        return cls(news_csv, vectorizer)\n",
    "    @staticmethod\n",
    "    def load_vectorizer_only(vectorizer_filepath):\n",
    "        with open(vectorizer_filepath) as fp:\n",
    "            return NameVectorizer.from_serializable(json.load(fp))\n",
    "        \n",
    "    #保存vectorizer为json文件\n",
    "    def save_vectorizer(self, vectorizer_filepath):\n",
    "        with open(vectorizer_filepath, \"w\") as fp:\n",
    "            json.dump(self.vectorizer.to_serializable(), fp)\n",
    "            \n",
    "    def get_vectorizer(self):\n",
    "        return self.vectorizer\n",
    "    \n",
    "    #设置当前数据集\n",
    "    def set_split(self,split='train'):\n",
    "        self.target_split=split\n",
    "        #获取对应数据集的文本和标签\n",
    "        self.target_df,self.target_size=self.lookup_dict[split]\n",
    "    \n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.target_size\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        #获取idx的那一行数据\n",
    "        row=self.target_df.iloc[idx]\n",
    "        \n",
    "        title_vector=self.vectorizer.vectorize(row.title,self.max_seq_length)\n",
    "        \n",
    "        category_vector=self.vectorizer.category_vocab.lookup_token(row.category)\n",
    "        \n",
    "        return {\n",
    "            'x_data':title_vector,\n",
    "            'y_target':category_vector\n",
    "        }\n",
    "    #计算多少个batch\n",
    "    def get_num_batches(self,batch_size):\n",
    "        return len(self)//batch_size\n",
    "    \n",
    "#用来生成数据\n",
    "def generate_batches(dataset,batch_size,shuffle=True,drop_last=True,device='cpu'):\n",
    "        dataloader=DataLoader(dataset=dataset,batch_size=batch_size,shuffle=shuffle,drop_last=drop_last)\n",
    "        \n",
    "        for data_dict in dataloader:\n",
    "            out_data_dict={}\n",
    "            \n",
    "            for name,tensor in data_dict.items():\n",
    "                out_data_dict[name]=data_dict[name].to(device)\n",
    "            yield out_data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#模型定义\n",
    "\n",
    "class NewsClassifier(nn.Module):\n",
    "    \n",
    "    def __init__(self,embedding_size,num_embeddings,\n",
    "                 num_channels,hidden_dim,num_classes,dropout_p,pretrained_embeddings=None,padding_idx=0):\n",
    "        \"\"\"\n",
    "        embedding_size: 词向量的长度\n",
    "        num_embedidngs：词向量的数量\n",
    "        num_channels:每层卷积的数量\n",
    "        hidden_dim:隐藏层的数目\n",
    "        num_classes:类别数目\n",
    "        \"\"\"\n",
    "        \n",
    "        super(NewsClassifier,self).__init__()\n",
    "        \n",
    "        if pretrained_embeddings is None:\n",
    "            self.emb=nn.Embedding(embedding_dim=embedding_size,num_embeddings=num_embeddings,padding_idx=padding_idx)\n",
    "        else:\n",
    "            pretrained_embeddings=torch.from_numpy(pretrained_embeddings).float()\n",
    "            #给予权重\n",
    "            self.emb=nn.Embedding(embedding_dim=embedding_size,num_embeddings=num_embeddings,padding_idx=padding_idx,_weight=pretrained_embeddings)\n",
    "        \n",
    "        self.conv=nn.Sequential(\n",
    "            nn.Conv1d(embedding_size,num_channels,3),\n",
    "            nn.ELU(),\n",
    "            nn.Conv1d(num_channels,num_channels,3,stride=2),\n",
    "            nn.ELU(),\n",
    "            nn.Conv1d(num_channels,num_channels,3,stride=2),\n",
    "            nn.ELU(),\n",
    "            nn.Conv1d(num_channels,num_channels,3),\n",
    "            nn.ELU()\n",
    "        )\n",
    "        \n",
    "        self.dropout_p=dropout_p\n",
    "        \n",
    "        self.fc1=nn.Linear(num_channels,hidden_dim)\n",
    "        self.fc2=nn.Linear(hidden_dim,num_classes)\n",
    "    \n",
    "    def  forward(self,x_in,apply_softmax=False):\n",
    "        \n",
    "        #permute 变化size\n",
    "        x_embedded=self.emb(x_in).permute(0,2,1)\n",
    "        features=self.conv(x_embedded)\n",
    "        \n",
    "        remaining_size=features.size(dim=2)\n",
    "        features=F.avg_pool1d(features,remaining_size).squeeze(dim=2)\n",
    "        features = F.dropout(features, p=self.dropout_p)\n",
    "        \n",
    "        intermediate_vector = F.relu(F.dropout(self.fc1(features), p=self.dropout_p))\n",
    "        prediction_vector = self.fc2(intermediate_vector)\n",
    "        \n",
    "        if apply_softmax:\n",
    "            prediction_vector = F.softmax(prediction_vector, dim=1)\n",
    "        return prediction_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_train_state(args):\n",
    "    return {'stop_early': False,\n",
    "            'early_stopping_step': 0,\n",
    "            'early_stopping_best_val': 1e8,\n",
    "            'learning_rate': args.learning_rate,\n",
    "            'epoch_index': 0,\n",
    "            'train_loss': [],\n",
    "            'train_acc': [],\n",
    "            'val_loss': [],\n",
    "            'val_acc': [],\n",
    "            'test_loss': -1,\n",
    "            'test_acc': -1,\n",
    "            'model_filename': args.model_state_file}\n",
    "def update_train_state(args, model, train_state):\n",
    "    \n",
    "    # Save one model at least\n",
    "    if train_state['epoch_index'] == 0:\n",
    "        torch.save(model.state_dict(), train_state['model_filename'])\n",
    "        train_state['stop_early'] = False\n",
    "\n",
    "    # Save model if performance improved\n",
    "    elif train_state['epoch_index'] >= 1:\n",
    "        loss_tm1, loss_t = train_state['val_loss'][-2:]\n",
    "\n",
    "        # If loss worsened\n",
    "        if loss_t >= train_state['early_stopping_best_val']:\n",
    "            # Update step\n",
    "            train_state['early_stopping_step'] += 1\n",
    "        # Loss decreased\n",
    "        else:\n",
    "            # Save the best model\n",
    "            if loss_t < train_state['early_stopping_best_val']:\n",
    "                torch.save(model.state_dict(), train_state['model_filename'])\n",
    "\n",
    "            # Reset early stopping step\n",
    "            train_state['early_stopping_step'] = 0\n",
    "\n",
    "        # Stop early ?\n",
    "        train_state['stop_early'] = train_state['early_stopping_step'] >= args.early_stopping_criteria\n",
    "\n",
    "    return train_state\n",
    "def compute_accuracy(y_pred, y_target):\n",
    "    _, y_pred_indices = y_pred.max(dim=1)\n",
    "    n_correct = torch.eq(y_pred_indices, y_target).sum().item()\n",
    "    return n_correct / len(y_pred_indices) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed_everywhere(seed, cuda):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if cuda:\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "def handle_dirs(dirpath):\n",
    "    if not os.path.exists(dirpath):\n",
    "        os.makedirs(dirpath)\n",
    "\n",
    "def load_glove_from_file(glove_filepath):\n",
    "    \n",
    "    word_to_index={}\n",
    "    embeddings=[]\n",
    "    \n",
    "    with open(glove_filepath,'r',encoding='utf-8') as fp:\n",
    "        for index,line in enumerate(fp):\n",
    "            line=line.split(' ')\n",
    "            word_to_index[line[0]]=index\n",
    "            embedding_i=np.array([float(val) for val in line[1:]])\n",
    "            embeddings.append(embedding_i)\n",
    "        return word_to_index,np.stack(embeddings)\n",
    "def make_embedding_matrix(glove_filepath, words):\n",
    "    word_to_idx, glove_embeddings = load_glove_from_file(glove_filepath)\n",
    "    embedding_size=glove_embeddings.shape[1]\n",
    "    \n",
    "    final_embeddings=np.zeros((len(words),embedding_size))\n",
    "    \n",
    "    for i,word in enumerate(words):\n",
    "        if word in word_to_idx:\n",
    "            final_embeddings[i, :] = glove_embeddings[word_to_idx[word]]\n",
    "        else:\n",
    "            embedding_i = torch.ones(1, embedding_size)\n",
    "            torch.nn.init.xavier_uniform_(embedding_i)\n",
    "            final_embeddings[i, :] = embedding_i\n",
    "\n",
    "    return final_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expanded filepaths: \n",
      "\tmodel_storage/document_classification\\vectorizer.json\n",
      "\tmodel_storage/document_classification\\model.pth\n",
      "Using CUDA: True\n"
     ]
    }
   ],
   "source": [
    "from argparse import Namespace\n",
    "args = Namespace(\n",
    "    # Data and Path hyper parameters\n",
    "    news_csv=\"data/news_with_splits.csv\",\n",
    "    vectorizer_file=\"vectorizer.json\",\n",
    "    model_state_file=\"model.pth\",\n",
    "    save_dir=\"model_storage/document_classification\",\n",
    "    # Model hyper parameters\n",
    "    glove_filepath='data/glove/glove.6B.100d.txt', \n",
    "    use_glove=False,\n",
    "    embedding_size=100, \n",
    "    hidden_dim=100, \n",
    "    num_channels=100, \n",
    "    # Training hyper parameter\n",
    "    seed=1337, \n",
    "    learning_rate=0.001, \n",
    "    dropout_p=0.1, \n",
    "    batch_size=128, \n",
    "    num_epochs=100, \n",
    "    early_stopping_criteria=5, \n",
    "    # Runtime option\n",
    "    cuda=True, \n",
    "    catch_keyboard_interrupt=True, \n",
    "    reload_from_files=False,\n",
    "    expand_filepaths_to_save_dir=True\n",
    ") \n",
    "if args.expand_filepaths_to_save_dir:\n",
    "    args.vectorizer_file = os.path.join(args.save_dir,\n",
    "                                        args.vectorizer_file)\n",
    "\n",
    "    args.model_state_file = os.path.join(args.save_dir,\n",
    "                                         args.model_state_file)\n",
    "    \n",
    "    print(\"Expanded filepaths: \")\n",
    "    print(\"\\t{}\".format(args.vectorizer_file))\n",
    "    print(\"\\t{}\".format(args.model_state_file))\n",
    "    \n",
    "# Check CUDA\n",
    "if not torch.cuda.is_available():\n",
    "    args.cuda = False\n",
    "    \n",
    "args.device = torch.device(\"cuda\" if args.cuda else \"cpu\")\n",
    "print(\"Using CUDA: {}\".format(args.cuda))\n",
    "\n",
    "# Set seed for reproducibility\n",
    "set_seed_everywhere(args.seed, args.cuda)\n",
    "\n",
    "# handle dirs\n",
    "handle_dirs(args.save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using pre-trained embeddings\n"
     ]
    }
   ],
   "source": [
    "args.use_glove = True\n",
    "#如果是重新加载\n",
    "if args.reload_from_files:\n",
    "    dataset = NewsDataset.load_dataset_and_load_vectorizer(args.news_csv,\n",
    "                                                              args.vectorizer_file)\n",
    "else:\n",
    "    #创造数据库\n",
    "    dataset = NewsDataset.load_dataset_and_make_vectorizer(args.news_csv)\n",
    "    dataset.save_vectorizer(args.vectorizer_file)\n",
    "#获取向量化工具类\n",
    "vectorizer = dataset.get_vectorizer()\n",
    "\n",
    "#判断是否使用预训练词向量\n",
    "if args.use_glove:\n",
    "    words=vectorizer.title_vocab.token_to_idx.keys()\n",
    "    embeddings=make_embedding_matrix(args.glove_filepath,words=words)\n",
    "    print(\"Using pre-trained embeddings\")\n",
    "else:\n",
    "    print(\"Not using pre-trained embeddings\")\n",
    "    embeddings = None\n",
    "\n",
    "    \n",
    "classifier = NewsClassifier(embedding_size=args.embedding_size, \n",
    "                            num_embeddings=len(vectorizer.title_vocab),\n",
    "                            num_channels=args.num_channels,\n",
    "                            hidden_dim=args.hidden_dim, \n",
    "                            num_classes=len(vectorizer.category_vocab), \n",
    "                            dropout_p=args.dropout_p,\n",
    "                            pretrained_embeddings=embeddings,\n",
    "                            padding_idx=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3297"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vectorizer.title_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0ec5ebc14c04b7a99d5048ed15bb0bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='training routine', style=ProgressStyle(description_width='ini…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "538f36637ec44d95b03d02884d4fe6d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='split=train', max=656, style=ProgressStyle(description_width=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "434feb215d1b4b0e8be6b8ee46a30267",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='split=val', max=140, style=ProgressStyle(description_width='i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "classifier = classifier.to(args.device)\n",
    "dataset.class_weights = dataset.class_weights.to(args.device)\n",
    "    \n",
    "loss_func = nn.CrossEntropyLoss(dataset.class_weights)\n",
    "optimizer = optim.Adam(classifier.parameters(), lr=args.learning_rate)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer,\n",
    "                                           mode='min', factor=0.5,\n",
    "                                           patience=1)\n",
    "\n",
    "train_state = make_train_state(args)\n",
    "\n",
    "epoch_bar = tqdm_notebook(desc='training routine', \n",
    "                          total=args.num_epochs,\n",
    "                          position=0)\n",
    "\n",
    "dataset.set_split('train')\n",
    "train_bar = tqdm_notebook(desc='split=train',\n",
    "                          total=dataset.get_num_batches(args.batch_size), \n",
    "                          position=1, \n",
    "                          leave=True)\n",
    "dataset.set_split('val')\n",
    "val_bar = tqdm_notebook(desc='split=val',\n",
    "                        total=dataset.get_num_batches(args.batch_size), \n",
    "                        position=1, \n",
    "                        leave=True)\n",
    "try:\n",
    "    for epoch_index in range(args.num_epochs):\n",
    "        train_state['epoch_index'] = epoch_index\n",
    "\n",
    "        # Iterate over training dataset\n",
    "\n",
    "        # setup: batch generator, set loss and acc to 0, set train mode on\n",
    "\n",
    "        dataset.set_split('train')\n",
    "        batch_generator = generate_batches(dataset, \n",
    "                                           batch_size=args.batch_size, \n",
    "                                           device=args.device)\n",
    "        running_loss = 0.0\n",
    "        running_acc = 0.0\n",
    "        classifier.train()\n",
    "\n",
    "        for batch_index, batch_dict in enumerate(batch_generator):\n",
    "            # the training routine is these 5 steps:\n",
    "\n",
    "            # --------------------------------------\n",
    "            # step 1. zero the gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # step 2. compute the output\n",
    "            y_pred = classifier(batch_dict['x_data'])\n",
    "\n",
    "            # step 3. compute the loss\n",
    "            loss = loss_func(y_pred, batch_dict['y_target'])\n",
    "            loss_t = loss.item()\n",
    "            running_loss += (loss_t - running_loss) / (batch_index + 1)\n",
    "\n",
    "            # step 4. use loss to produce gradients\n",
    "            loss.backward()\n",
    "\n",
    "            # step 5. use optimizer to take gradient step\n",
    "            optimizer.step()\n",
    "            # -----------------------------------------\n",
    "            # compute the accuracy\n",
    "            acc_t = compute_accuracy(y_pred, batch_dict['y_target'])\n",
    "            running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
    "\n",
    "            # update bar\n",
    "            train_bar.set_postfix(loss=running_loss, acc=running_acc, \n",
    "                                  epoch=epoch_index)\n",
    "            train_bar.update()\n",
    "\n",
    "        train_state['train_loss'].append(running_loss)\n",
    "        train_state['train_acc'].append(running_acc)\n",
    "\n",
    "        # Iterate over val dataset\n",
    "\n",
    "        # setup: batch generator, set loss and acc to 0; set eval mode on\n",
    "        dataset.set_split('val')\n",
    "        batch_generator = generate_batches(dataset, \n",
    "                                           batch_size=args.batch_size, \n",
    "                                           device=args.device)\n",
    "        running_loss = 0.\n",
    "        running_acc = 0.\n",
    "        classifier.eval()\n",
    "\n",
    "        for batch_index, batch_dict in enumerate(batch_generator):\n",
    "            # compute the output\n",
    "            y_pred =  classifier(batch_dict['x_data'])\n",
    "\n",
    "            # step 3. compute the loss\n",
    "            loss = loss_func(y_pred, batch_dict['y_target'])\n",
    "            loss_t = loss.item()\n",
    "            running_loss += (loss_t - running_loss) / (batch_index + 1)\n",
    "\n",
    "            # compute the accuracy\n",
    "            acc_t = compute_accuracy(y_pred, batch_dict['y_target'])\n",
    "            running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
    "            val_bar.set_postfix(loss=running_loss, acc=running_acc, \n",
    "                            epoch=epoch_index)\n",
    "            val_bar.update()\n",
    "\n",
    "        train_state['val_loss'].append(running_loss)\n",
    "        train_state['val_acc'].append(running_acc)\n",
    "\n",
    "        train_state = update_train_state(args=args, model=classifier,\n",
    "                                         train_state=train_state)\n",
    "\n",
    "        scheduler.step(train_state['val_loss'][-1])\n",
    "\n",
    "        if train_state['stop_early']:\n",
    "            break\n",
    "\n",
    "        train_bar.n = 0\n",
    "        val_bar.n = 0\n",
    "        epoch_bar.update()\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Exiting loop\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.load_state_dict(torch.load(train_state['model_filename']))\n",
    "\n",
    "classifier = classifier.to(args.device)\n",
    "dataset.class_weights = dataset.class_weights.to(args.device)\n",
    "loss_func = nn.CrossEntropyLoss(dataset.class_weights)\n",
    "\n",
    "dataset.set_split('test')\n",
    "batch_generator = generate_batches(dataset, \n",
    "                                   batch_size=args.batch_size, \n",
    "                                   device=args.device)\n",
    "running_loss = 0.\n",
    "running_acc = 0.\n",
    "classifier.eval()\n",
    "\n",
    "for batch_index, batch_dict in enumerate(batch_generator):\n",
    "    # compute the output\n",
    "    y_pred =  classifier(batch_dict['x_data'])\n",
    "    \n",
    "    # compute the loss\n",
    "    loss = loss_func(y_pred, batch_dict['y_target'])\n",
    "    loss_t = loss.item()\n",
    "    running_loss += (loss_t - running_loss) / (batch_index + 1)\n",
    "\n",
    "    # compute the accuracy\n",
    "    acc_t = compute_accuracy(y_pred, batch_dict['y_target'])\n",
    "    running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
    "\n",
    "train_state['test_loss'] = running_loss\n",
    "train_state['test_acc'] = running_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.5621352142521316;\n",
      "Test Accuracy: 82.91852678571425\n"
     ]
    }
   ],
   "source": [
    "print(\"Test loss: {};\".format(train_state['test_loss']))\n",
    "print(\"Test Accuracy: {}\".format(train_state['test_acc']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    text = ' '.join(word.lower() for word in text.split(\" \"))\n",
    "    text = re.sub(r\"([.,!?])\", r\" \\1 \", text)\n",
    "    text = re.sub(r\"[^a-zA-Z.,!?]+\", r\" \", text)\n",
    "    return text\n",
    "#用来推断信\n",
    "def predict_category(title, classifier, vectorizer, max_length):"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
